#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import yaml
import requests
import re
import sys
from collections import defaultdict
import json
import jsonschema

# ==================== é…ç½®åŒº ====================
ORIGINAL_YAML = r"D:\Database\Project\Config\clash_self.yaml"        # åŸå§‹ Clash é…ç½®æ–‡ä»¶è·¯å¾„
CUSTOM_INI = r"D:\Database\Project\Config\self_config.ini"            # è‡ªå®šä¹‰è§„åˆ™é…ç½®æ–‡ä»¶è·¯å¾„
OUTPUT_YAML = r"D:\Database\Project\Config\self_conf_new.yaml"      # è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯æ”¹ä¸ºè¦†ç›–åŸæ–‡ä»¶ï¼‰
SCHEMA_FILE   = r"D:\Database\Project\Config\meta-json-schema.json"
TIMEOUT = 15                         # ä¸‹è½½è§„åˆ™è¶…æ—¶æ—¶é—´
# ================================================

# headers = {
#     #"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
#     "User-Agent": "Clash"
# }
headers = {"User-Agent": "Clash Meta"}

def load_yaml(path):
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def save_yaml(data, path):
    with open(path, 'w', encoding='utf-8') as f:
        yaml.dump(data, f, allow_unicode=True, sort_keys=False, indent=2, width=9999)

def download_text(url):
    try:
        r = requests.get(url, headers=headers, timeout=TIMEOUT)
        r.raise_for_status()
        return r.text
    except Exception as e:
        print(f"[-] ä¸‹è½½å¤±è´¥ {url}: {e}")
        return ""

def extract_payload(text):
    """æ”¯æŒ clash-classic / ios_rule_script çš„ payload æ ¼å¼"""
    try:
        data = yaml.safe_load(text)
        payload = data.get("payload", [])
        rules = []
        for item in payload:
            if isinstance(item, str):
                rule = item.strip()
                if rule and not rule.startswith('#'):
                    rule = re.sub(r'^\s*-\s*', '', rule)  # å»æ‰å¼€å¤´çš„ -
                    rules.append(rule)
        return rules
    except:
        return []

def download_ruleset(source):
    if not source.startswith("http"):
        return []
    text = download_text(source)
    if "payload:" in text.lower():
        return extract_payload(text)
    # æ™®é€šçº¯æ–‡æœ¬è§„åˆ™
    return [line.strip() for line in text.splitlines()
            if line.strip() and not line.startswith(('#', ';'))]

def parse_custom_ini(path):
    rulesets = defaultdict(list)   # group_name -> [source1, source2, ...]
    proxy_groups = []

    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith(('#', ';')):
                continue
            if line.startswith('ruleset='):
                val = line[8:].strip()
                if ',' not in val: continue
                group, src = [x.strip() for x in val.split(',', 1)]
                src = re.sub(r'^(clash|mihomo|clash-classic):', '', src)
                rulesets[group].append(src)
            elif line.startswith('custom_proxy_group='):
                proxy_groups.append(line[19:].strip())

    return rulesets, proxy_groups

def generate_proxy_groups(defs, all_proxy_names):
    groups = []
    for definition in defs:
        parts = [p.strip() for p in definition.split('`')]
        name = parts[0]
        gtype = parts[1].lower()
        g = {"name": name, "type": gtype}

        if gtype == "select":
            proxies = []
            for p in parts[2:]:
                if p.startswith('[]'):
                    target = p[2:]
                    proxies.append("DIRECT" if target == "DIRECT" else target)
                else:
                    proxies.append(p)
            g["proxies"] = proxies

        elif gtype in ["url-test", "fallback", "load-balance"]:
            proxies = []
            url = "http://www.gstatic.com/generate_204"
            interval = 300
            tolerance = 0

            for p in parts[2:]:
                if p.startswith('[]'):
                    target = p[2:]
                    proxies.append("DIRECT" if target == "DIRECT" else target)
                elif p.startswith('(') and p.endswith(')'):
                    pattern = p[1:-1]
                    matched = [n for n in all_proxy_names if re.search(pattern, n, re.I)]
                    proxies.extend(matched)
                elif p.startswith('http'):
                    url = p
                elif ',' in p:
                    i, t = map(str.strip, p.split(',', 1))
                    interval = int(i)
                    if t: tolerance = int(t)
                else:
                    try: interval = int(p)
                    except: pass

            g["proxies"] = proxies or ["DIRECT"]
            g["url"] = url
            g["interval"] = interval
            if tolerance: g["tolerance"] = tolerance

        groups.append(g)
    return groups

def validate_config(config_dict):
    """éªŒè¯é…ç½®æ˜¯å¦ç¬¦åˆ meta-json-schema.json"""
    with open(SCHEMA_FILE, 'r', encoding='utf-8') as f:
        schema = json.load(f)
    jsonschema.validate(instance=config_dict, schema=schema)
    print("[+] é…ç½®éªŒè¯é€šè¿‡ meta-json-schema.json")

def main():
    print("[+] æ­£åœ¨åŠ è½½åŸå§‹é…ç½®æ–‡ä»¶...")
    config = load_yaml(ORIGINAL_YAML)
    proxies = config.get("proxies", [])
    proxy_names = [p.get("name") for p in proxies if p.get("name")]

    print(f"[+] å‘ç° {len(proxy_names)} ä¸ªèŠ‚ç‚¹")

    rulesets, pg_defs = parse_custom_ini(CUSTOM_INI)

    new_rules = []

    print("[+] å¤„ç†è§„åˆ™é›†...")
    for group_name, sources in rulesets.items():
        print(f"  â†’ {group_name} â† {len(sources)} ä¸ªæ¥æº")
        for src in sources:
            if src.upper().startswith("[]GEOIP,"):
                country = src.split(",", 1)[1].strip().upper()
                new_rules.append(f"GEOIP,{country},{group_name}")
                continue

            lines = download_ruleset(src)
            for line in lines:
                line = line.strip()
                if not line: continue

                if re.match(r'^(USER-AGENT)', line, re.I):
                    continue

                if re.match(r'^(MATCH|FINAL|GEOIP|AND|OR|NOT)', line, re.I):
                    new_rules.append(line)
                    continue

                if ',' in line:
                    parts = line.split(",", 2)
                    rule_type = parts[0].strip()
                    payload = parts[1].strip()
                    extra = "," + parts[2] if len(parts) > 2 else ""
                    new_rules.append(f"{rule_type},{payload},{group_name}{extra}")
                else:
                    new_rules.append(f"{line},{group_name}")

    new_rules.extend([
        "GEOIP,CN,ğŸ¯ ç›´è¿",
        "MATCH,ğŸŸ æ¼ç½‘ä¹‹é±¼"
    ])

    print("[+] ç”Ÿæˆ proxy-groups...")
    new_groups = generate_proxy_groups(pg_defs, proxy_names)

    result = config.copy()
    result["rules"] = new_rules
    result["proxy-groups"] = new_groups

    print("[+] éªŒè¯ç”Ÿæˆçš„é…ç½®...")

    save_yaml(result, OUTPUT_YAML)

    validate_config(result)

    
    print(f"[+] å®Œæˆï¼æ–°é…ç½®æ–‡ä»¶å·²ä¿å­˜ï¼š{OUTPUT_YAML}")

if __name__ == "__main__":
    main()